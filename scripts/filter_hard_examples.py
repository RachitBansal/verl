#!/usr/bin/env python3
"""
Filter Hard Examples from Evaluation Predictions

This script filters examples from the predictions parquet file (generated by 
evaluate_olmo2_math.sh with n_samples=128) to find "hard" examples where
the model achieves low accuracy (e.g., around 1/64).

Usage:p
    python scripts/filter_hard_examples.py \
        --input_file /path/to/gsm8k_predictions.parquet \
        --output_dir /path/to/output \
        --target_accuracy 0.015625 \
        --tolerance 0.03

The input file should be a predictions parquet with:
    - responses: list of N responses per example
    - data_source: dataset identifier (e.g., 'openai/gsm8k')
    - reward_model: dict with 'ground_truth' key

Author: Sunny + Claude
"""

import argparse
import os
import sys
from pathlib import Path

import numpy as np
import pandas as pd
from tqdm import tqdm

# Add verl to path
SCRIPT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(SCRIPT_DIR))


def parse_args():
    parser = argparse.ArgumentParser(
        description="Filter hard examples from evaluation predictions"
    )
    
    # Input/Output configuration
    parser.add_argument(
        "--input_file",
        type=str,
        required=True,
        help="Path to predictions parquet (from evaluate_olmo2_math.sh)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        required=True,
        help="Directory to save filtered hard examples"
    )
    
    # Hard example selection
    parser.add_argument(
        "--target_accuracy",
        type=float,
        default=1/64,  # ~0.015625
        help="Target accuracy for hard examples (default: 1/64)"
    )
    parser.add_argument(
        "--tolerance",
        type=float,
        default=0.03,
        help="Tolerance around target accuracy (e.g., 0.03 means 1/64 ± 3%%)"
    )
    parser.add_argument(
        "--n_hard_examples",
        type=int,
        default=None,
        help="Maximum number of hard examples to collect (None = all matching)"
    )
    
    # Optional: use original dataset for output (preserves all columns)
    parser.add_argument(
        "--original_dataset",
        type=str,
        default=None,
        help="Path to original dataset (for preserving all columns in output)"
    )
    
    # Scoring configuration
    parser.add_argument(
        "--score_threshold",
        type=float,
        default=0.5,
        help="Score threshold for correct answers (default: 0.5)"
    )
    
    # Output options
    parser.add_argument(
        "--save_all_scores",
        action="store_true",
        help="Save accuracy scores for all examples (for analysis)"
    )
    parser.add_argument(
        "--train_split_ratio",
        type=float,
        default=0.9,
        help="Ratio for train/val split (default: 0.9)"
    )
    
    return parser.parse_args()


def load_reward_function():
    """Load the reward/scoring function for math problems."""
    from recipe.open_math_reasoning.compute_score import compute_score_data_source
    return compute_score_data_source


def compute_example_accuracy(
    responses: list,
    ground_truth: str,
    data_source: str,
    compute_score_fn,
    score_threshold: float = 0.5,
) -> dict:
    """
    Compute accuracy for a single example across all its responses.
    
    Returns dict with:
        - n_correct: number of correct responses
        - n_total: total number of responses  
        - accuracy: n_correct / n_total
        - scores: list of individual scores (for debugging)
    """
    n_correct = 0
    n_total = len(responses)
    scores = []
    
    for response in responses:
        try:
            score = compute_score_fn(data_source, response, ground_truth)
        except Exception as e:
            # Some scoring functions may fail on malformed responses
            score = 0.0
        
        scores.append(score)
        if score >= score_threshold:
            n_correct += 1
    
    accuracy = n_correct / n_total if n_total > 0 else 0.0
    
    return {
        "n_correct": n_correct,
        "n_total": n_total,
        "accuracy": accuracy,
        "scores": scores,
    }


def main():
    args = parse_args()
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("=" * 60)
    print("Filtering Hard Examples from Predictions")
    print("=" * 60)
    print(f"Input: {args.input_file}")
    print(f"Output: {args.output_dir}")
    print(f"Target accuracy: {args.target_accuracy:.4f} (1/{int(1/args.target_accuracy) if args.target_accuracy > 0 else 'inf'})")
    print(f"Tolerance: ±{args.tolerance:.4f}")
    print(f"Acceptance range: [{args.target_accuracy - args.tolerance:.4f}, {args.target_accuracy + args.tolerance:.4f}]")
    if args.n_hard_examples:
        print(f"Max hard examples: {args.n_hard_examples}")
    print("=" * 60)
    
    # Load predictions
    print("\nLoading predictions...")
    df = pd.read_parquet(args.input_file)
    print(f"Loaded {len(df)} examples")
    
    # Check required columns
    required_cols = ["responses", "data_source", "reward_model"]
    missing = [c for c in required_cols if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns: {missing}. Available: {df.columns.tolist()}")
    
    # Get sample counts
    sample_counts = df["responses"].apply(len)
    n_samples = sample_counts.iloc[0]
    print(f"Responses per example: {n_samples}")
    
    if not sample_counts.eq(n_samples).all():
        print(f"WARNING: Variable response counts: min={sample_counts.min()}, max={sample_counts.max()}")
    
    # Load scoring function
    print("\nLoading scoring function...")
    compute_score_fn = load_reward_function()
    
    # Compute accuracies for all examples
    print("\nComputing per-example accuracies...")
    accuracies = []
    n_corrects = []
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Scoring"):
        responses = row["responses"]
        data_source = row["data_source"]
        reward_data = row["reward_model"]
        
        # Extract ground truth
        if isinstance(reward_data, dict):
            ground_truth = reward_data.get("ground_truth", "")
        else:
            ground_truth = str(reward_data)
        
        result = compute_example_accuracy(
            responses=responses,
            ground_truth=ground_truth,
            data_source=data_source,
            compute_score_fn=compute_score_fn,
            score_threshold=args.score_threshold,
        )
        
        accuracies.append(result["accuracy"])
        n_corrects.append(result["n_correct"])
    
    df["measured_accuracy"] = accuracies
    df["n_correct"] = n_corrects
    df["n_total"] = n_samples
    
    # Compute and print statistics
    print(f"\n{'=' * 60}")
    print("Overall Statistics")
    print(f"{'=' * 60}")
    print(f"Total examples: {len(df)}")
    print(f"Mean accuracy: {np.mean(accuracies):.4f}")
    print(f"Median accuracy: {np.median(accuracies):.4f}")
    print(f"Std accuracy: {np.std(accuracies):.4f}")
    print(f"Pass@{n_samples}: {np.mean([a > 0 for a in accuracies]):.4f}")
    
    # Print accuracy distribution
    print(f"\nAccuracy distribution:")
    bins = [0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 1.01]
    labels = ["[0, 0.01)", "[0.01, 0.02)", "[0.02, 0.05)", "[0.05, 0.1)", 
              "[0.1, 0.2)", "[0.2, 0.5)", "[0.5, 1.0)", "[1.0]"]
    for i in range(len(bins) - 1):
        count = ((np.array(accuracies) >= bins[i]) & (np.array(accuracies) < bins[i+1])).sum()
        pct = count / len(accuracies) * 100
        print(f"  {labels[i]:15s}: {count:6d} ({pct:5.1f}%)")
    
    # Filter hard examples
    acc_lower = args.target_accuracy - args.tolerance
    acc_upper = args.target_accuracy + args.tolerance
    
    hard_mask = (df["measured_accuracy"] >= acc_lower) & (df["measured_accuracy"] <= acc_upper)
    hard_df = df[hard_mask].copy()
    
    if args.n_hard_examples and len(hard_df) > args.n_hard_examples:
        # Randomly sample if we have too many
        hard_df = hard_df.sample(n=args.n_hard_examples, random_state=42)
    
    print(f"\n{'=' * 60}")
    print("Hard Examples")
    print(f"{'=' * 60}")
    print(f"Found {len(hard_df)} examples with accuracy in [{acc_lower:.4f}, {acc_upper:.4f}]")
    
    if len(hard_df) == 0:
        print("\nWARNING: No hard examples found! Consider:")
        print("  - Increasing --tolerance")
        print("  - Adjusting --target_accuracy")
        print(f"\nAccuracy percentiles:")
        for p in [1, 5, 10, 25, 50]:
            print(f"  {p}th percentile: {np.percentile(accuracies, p):.4f}")
        return
    
    # Print hard examples statistics
    print(f"\nHard examples statistics:")
    print(f"  Mean accuracy: {hard_df['measured_accuracy'].mean():.4f}")
    print(f"  Std accuracy: {hard_df['measured_accuracy'].std():.4f}")
    print(f"  Min accuracy: {hard_df['measured_accuracy'].min():.4f}")
    print(f"  Max accuracy: {hard_df['measured_accuracy'].max():.4f}")
    
    # Check data source distribution
    print(f"\nData source distribution in hard examples:")
    source_counts = hard_df["data_source"].value_counts()
    for source, count in source_counts.items():
        print(f"  {source}: {count}")
    
    # Prepare output dataframe (drop responses to save space)
    output_df = hard_df.drop(columns=["responses"], errors="ignore")
    
    # If original dataset provided, merge to get all original columns
    if args.original_dataset and os.path.exists(args.original_dataset):
        print(f"\nMerging with original dataset: {args.original_dataset}")
        orig_df = pd.read_parquet(args.original_dataset)
        # Use index to match - assumes predictions file preserves row order
        hard_indices = hard_df.index.tolist()
        output_df = orig_df.iloc[hard_indices].copy()
        output_df["measured_accuracy"] = hard_df["measured_accuracy"].values
        output_df["n_correct"] = hard_df["n_correct"].values
        output_df["n_total"] = hard_df["n_total"].values
    
    # Save hard examples
    output_path = os.path.join(args.output_dir, "hard_examples.parquet")
    output_df.to_parquet(output_path, index=False)
    print(f"\nSaved {len(output_df)} hard examples to: {output_path}")
    
    # Create train/val split
    n_train = int(len(output_df) * args.train_split_ratio)
    
    # Shuffle before splitting
    output_df_shuffled = output_df.sample(frac=1, random_state=42).reset_index(drop=True)
    train_df = output_df_shuffled.iloc[:n_train]
    val_df = output_df_shuffled.iloc[n_train:]
    
    train_path = os.path.join(args.output_dir, "train.parquet")
    val_path = os.path.join(args.output_dir, "val.parquet")
    
    train_df.to_parquet(train_path, index=False)
    val_df.to_parquet(val_path, index=False)
    
    print(f"Saved train split ({len(train_df)} examples): {train_path}")
    print(f"Saved val split ({len(val_df)} examples): {val_path}")
    
    # Save all scores if requested
    if args.save_all_scores:
        scores_df = df[["data_source", "measured_accuracy", "n_correct", "n_total"]].copy()
        scores_path = os.path.join(args.output_dir, "all_accuracy_scores.parquet")
        scores_df.to_parquet(scores_path, index=False)
        print(f"\nSaved all accuracy scores to: {scores_path}")
    
    print(f"\n{'=' * 60}")
    print("Done!")
    print(f"{'=' * 60}")


if __name__ == "__main__":
    main()

